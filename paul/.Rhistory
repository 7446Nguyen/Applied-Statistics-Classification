dfBartlett <- data.frame(WilksDegreesofFreedom, Bartlett_ChiSq$wilks, Bartletts_p)
colnames(dfBartlett) <- c("Chi-Square Statistic", "Degrees of Freedom", "Wilks' Lambda", "p-Value")
dfBartlett$`Chi-Square Statistic` <- round(as.numeric(as.character(dfBartlett$`Chi-Square Statistic`)), digits=5)
dfBartlett$`Wilks' Lambda` <- round(as.numeric(as.character(dfBartlett$`Wilks' Lambda`)), digits=5)
bartlettsTest <- data.frame(rbind(dfBartlett$`Chi-Square Statistic`,dfBartlett$`Degrees of Freedom`,dfBartlett$`Wilks' Lambda`,Bartletts_p))
rownames(bartlettsTest) <- c("Chi Square Statistic","Degrees of Freedom","Wilks' Lambda","p-Value")
colnames(bartlettsTest) <- "Statistics"
dfTrain <- df[which(!is.na(df$shot_made_flag)),]
prediction.Data <- df[which(is.na(df$shot_made_flag)),]
### Full Data train/test split for Logistic
test_sample_size <- floor(0.75 * nrow(dfTrain))
set.seed(123)
train_ind <- sample(seq_len(nrow(dfTrain)), size = test_sample_size)
subDF.Train <- dfTrain[train_ind, ] #75% training
subDF.Test <- dfTrain[-train_ind, ] # 25% testing
#### Numeric Data train/test split for QDA
test_sample_size <- floor(0.75 * nrow(dfTrain.numeric))
set.seed(123)
train_ind <- sample(seq_len(nrow(dfTrain.numeric)), size = test_sample_size)
subDF.Train.numeric <- dfTrain.numeric[train_ind, ] #75% training
subDF.Test.numeric <- dfTrain.numeric[-train_ind, ] # 25% testing
```
```{r Quadratic Discriminant Analysis: Test Production, echo=F, warning=F, include=F}
##################################################################################################
##################################################################################################
##################################################################################################
####### a Priori #######
# MASS package used for qda()
#df.numeric <- df.numeric[order(df.numeric$shot_made_flag),]
kobe.qda <- qda(shot_made_flag ~ ., CV=T, data=dfTrain.numeric)
data.frame(mean(kobe.qda$posterior[,1]), mean(kobe.qda$posterior[,2]))
shot_made_flag_Posterior <- rbind("0", "1")
proportion_Posterior <- rbind(mean(kobe.qda$posterior[,1]), mean(kobe.qda$posterior[,2]))
priori <- data.frame(shot_made_flag_Posterior, proportion_Posterior)
subDF.Train.numeric$shot_made_flag <- as.factor(subDF.Train.numeric$shot_made_flag)
#subDF.Train.numeric$shot_made_flag <- ifelse(subDF.Train.numeric$shot_made_flag=="1", "made", "not_made")
subDF.Train.numeric <- subDF.Train.numeric %>% mutate_if(is.integer, as.numeric) %>% mutate_if(is.character, as.factor) %>% data.frame()
# k=25 folds, repeat random folding for internal cross-validation 5 times:
train.Control <- caret::trainControl(method = "repeatedcv",
number = 25,
repeats = 5,
#summaryFunction = twoClassSummary,
summaryFunction = mnLogLoss,
classProbs = T) # classProbs=T to get mnLogLoss (also for twoClassSummary)
# build the model using the 75% partitioned from the internal dataset (the set with all shot_made_flag response results):
qda.filtered <- train(shot_made_flag ~ .
, data = subDF.Train.numeric
, method = "qda"
, trControl=train.Control
, preProcess = c("center", "scale", "spatialSign")
#, preProcess = "spatialSign"
, metric = "logLoss"
)
# test the model on the 25% partitions from the internal dataset:
internal_cv.predicted.qda <- suppressWarnings(predict(qda.filtered, newdata = subDF.Test.numeric))
# build a confusion matrix for internal cross-validation to see performance:
confusion_matrix_results.internal<- confusionMatrix(table(internal_cv.predicted.qda, subDF.Test.numeric$shot_made_flag))
external_cv.predicted.qda <- suppressWarnings(predict(qda.filtered, newdata = dfTrain.numeric))
# compare the predicted results to the actual results to make sure model still performs as intended:
confusion_matrix_results.external <- confusionMatrix(table(external_cv.predicted.qda, dfTrain.numeric$shot_made_flag))
SpecSense.confusion.internal <- data.frame(confusion_matrix_results.internal$byClass)
AccuracyP.confusion.internal <- data.frame(confusion_matrix_results.internal$overall)
Accuracy.confusion.internal <- AccuracyP.confusion.internal[1,]
Sensitivity.confusion.internal <- SpecSense.confusion.internal[1,]
Specificity.confusion.internal <- SpecSense.confusion.internal[2,]
Precision.confusion.internal <- SpecSense.confusion.internal[5,]
######### External Cross-Validation Metrics
SpecSense.confusion.external <- data.frame(confusion_matrix_results.external$byClass)
AccuracyP.confusion.external <- data.frame(confusion_matrix_results.external$overall)
Accuracy.confusion.external <- AccuracyP.confusion.external[1,]
Sensitivity.confusion.external <- SpecSense.confusion.external[1,]
Specificity.confusion.external <- SpecSense.confusion.external[2,]
Precision.confusion.external <- SpecSense.confusion.external[5,]
misclassification.QDA.external <- (confusion_matrix_results.external$table[2,1] + confusion_matrix_results.external$table[1,2]) / sum(confusion_matrix_results.external$table)
misclassification.QDA.internal <- (confusion_matrix_results.internal$table[2,1] + confusion_matrix_results.internal$table[1,2]) / sum(confusion_matrix_results.internal$table)
print(logLoss)
logLoss.quadratic <- qda.filtered$results[1,2]
logLoss_StDev <- qda.filtered$results[1,3]
dfTrain.numeric$shot_made_flag <- ifelse(dfTrain.numeric$shot_made_flag=="made",1,0)
### Internal Cross-Validation
internal_cv.predicted.qda <- ifelse(internal_cv.predicted.qda=="made",1,0)
AUCpredStep.internal <- prediction(internal_cv.predicted.qda, as.numeric(subDF.Test.numeric$shot_made_flag))
perf_step.internal <- performance(AUCpredStep.internal, measure = "tpr", x.measure = "fpr")
AUC.internal <- performance(AUCpredStep.internal, measure = "auc")
AUC.internal <- AUC.internal@y.values[[1]]
### External Cross-Validation
external_cv.predicted.qda <- ifelse(external_cv.predicted.qda=="made",1,0)
AUCpredStep.external <- prediction(external_cv.predicted.qda, as.numeric(dfTrain.numeric$shot_made_flag))
perf_step.external <- performance(AUCpredStep.external, measure = "tpr", x.measure = "fpr")
AUC.external <- performance(AUCpredStep.external, measure = "auc")
AUC.external <- AUC.external@y.values[[1]]
confusionFrame.internal <- data.frame(rbind(round(Sensitivity.confusion.internal, digits=5),round(Specificity.confusion.internal, digits=5),round(Precision.confusion.internal, digits=5),round(Accuracy.confusion.internal, digits=5),round(misclassification.QDA.internal, digits=5),round(logLoss.quadratic, digits=5),round(AUC.internal, digits=5)))
rownames(confusionFrame.internal) <- c("Sensitivity","Specificity","Precision","Accuracy","Misclassification Rate","Logarithmic Loss","Area Under the Curve")
### External Cross-Validation Confusion Matrix
confusionFrame.external <- data.frame(rbind(round(Sensitivity.confusion.external, digits=5),round(Specificity.confusion.external, digits=5),round(Precision.confusion.external, digits=5),round(Accuracy.confusion.external, digits=5),round(misclassification.QDA.external, digits=5),round(logLoss.quadratic, digits=5),round(AUC.external, digits=5)))
rownames(confusionFrame.external) <- c("Sensitivity","Specificity","Precision","Accuracy","Misclassification Rate","Logarithmic Loss","Area Under the Curve")
confusionFrame <- data.frame(confusionFrame.internal, confusionFrame.external)
colnames(confusionFrame) <- c("Internal CV Statistics", "External CV Statistics")
kable(confusionFrame, format="latex", booktabs = T)  %>%
kable_styling(latex_options="striped", position = "center")
pred.qda.filtered <- suppressWarnings(predict(qda.filtered, newdata = df.numeric.preds))
df.preds$shot_made_flag <- pred.qda.filtered
model.forward.Start <- glm(shot_made_flag~1, family=binomial(link='logit'), data = dfTrain)
model.Allvar <- glm(shot_made_flag ~ ., family=binomial(link='logit'), data = dfTrain)
########################################################################
#### Forward Selection
model.Forward <- stepAIC(model.forward.Start, direction = "forward", trace = F, scope = formula(model.Allvar))
summary(model.Forward)
model.Forward$anova
#################################### Forward Selection Model Suggestion
forward.glm <- glm(shot_made_flag ~ action_type + attendance + arena_temp +
game_event_id + season + seconds_remaining + minutes_remaining +
loc_y + game_date + loc_x + playoffs
, family=binomial(link='logit')
, data=dfTrain)
summary(forward.glm)
########################################################################
# Backward Elimination
model.Backward <- stepAIC(model.Allvar, direction = "backward", trace = F, scope = formula(model.forward.Start))
summary(model.Backward)
model.Backward$anova
#################################### Backward Elimination Model Suggestion
back.glm <- glm(shot_made_flag ~ recId + action_type + game_event_id +
loc_x + minutes_remaining + season + seconds_remaining +
shot_distance + game_date + shot_id + attendance + arena_temp
, family=binomial(link='logit')
, data=dfTrain)
summary(back.glm)
back.glm$aic
########################################################################
# Stepwise Regression
model.Stepwise <- stepAIC(model.Allvar, direction = "both", trace = F)
summary(model.Stepwise)
model.Stepwise$anova
#################################### Stepwise Regression Model Suggestion
step.glm <- glm(shot_made_flag ~ recId + action_type + game_event_id +
loc_x + minutes_remaining + season + seconds_remaining +
shot_distance + game_date + shot_id + attendance + arena_temp
, family=binomial(link='logit')
, data=dfTrain)
summary(step.glm)
step.glm$aic
model_stats <- data.frame()
model_stats$AIC <- data.frame(forward.glm$aic,back.glm$aic,step.glm$aic)
model_stats$ResDev <- data.frame(forward.glm$deviance,back.glm$deviance,step.glm$deviance)
colnames(model_stats) <- c("Selection Type", "AIC", "Residual Deviance")
rownames(model_stats) <- c("Forwards","Backwards","Stepwise")
kable(model_stats,format="latex", booktabs = T)  %>%
kable_styling(latex_options="striped", position = "center")
model_stats <- data.frame()
model_stats$AIC <- data.frame(forward.glm$aic,back.glm$aic,step.glm$aic)
model_stats$ResDev <- data.frame(forward.glm$deviance,back.glm$deviance,step.glm$deviance)
colnames(model_stats) <- c("Selection Type", "AIC", "Residual Deviance")
rownames(model_stats) <- c("Forwards","Backwards","Stepwise")
model_stats <- data.frame()
model_stats$AIC <- data.frame(forward.glm$aic,back.glm$aic,step.glm$aic)
model_stats$AIC <- c(forward.glm$aic,back.glm$aic,step.glm$aic)
model_stats$AIC <- rbind(forward.glm$aic,back.glm$aic,step.glm$aic)
model_stats <- data.frame(forward.glm$aic,back.glm$aic,step.glm$aic)
model_stats$ResDev <- c(forward.glm$deviance,back.glm$deviance,step.glm$deviance)
data.frame(cbind(rbind(forward.glm$aic,back.glm$aic,step.glm$aic),rbind(forward.glm$deviance,back.glm$deviance,step.glm$deviance)))
model_stats <- data.frame(cbind(rbind(forward.glm$aic,back.glm$aic,step.glm$aic),rbind(forward.glm$deviance,back.glm$deviance,step.glm$deviance)))
colnames(model_stats) <- c("Selection Type", "AIC", "Residual Deviance")
model_stats <- data.frame(cbind(rbind("Forwards","Backwards","Stepwise"),rbind(forward.glm$aic,back.glm$aic,step.glm$aic),rbind(forward.glm$deviance,back.glm$deviance,step.glm$deviance)))
model_stats
colnames(model_stats) <- c("Selection Type", "AIC", "Residual Deviance")
model_stats
kable(model_stats,format="latex", booktabs = T)  %>%
kable_styling(latex_options="striped", position = "center")
kable(model_stats,format="markdown", booktabs = T)  %>%
kable_styling(latex_options="striped", position = "center")
model_stats <- data.frame(cbind(rbind("Forwards","Backwards","Stepwise"),rbind(round(forward.glm$aic, digits=5),round(back.glm$aic,step.glm$aic, digits=5)),rbind(round(forward.glm$deviance,back.glm$deviance, digits=5),round(step.glm$deviance, digits=5))))
data.frame(cbind(rbind("Forwards","Backwards","Stepwise"),rbind(round(forward.glm$aic, digits=5),round(back.glm$aic,step.glm$aic, digits=5)),rbind(round(forward.glm$deviance,back.glm$deviance, digits=5),round(step.glm$deviance, digits=5))))
rbind(round(forward.glm$aic, digits=5),round(back.glm$aic,step.glm$aic, digits=5))
round(forward.glm$aic, digits=5)
round(back.glm$aic,step.glm$aic, digits=5)
model_stats <- data.frame(cbind(rbind("Forwards","Backwards","Stepwise"),rbind(round(forward.glm$aic, digits=5),round(back.glm$aic, digits=5),round(step.glm$aic, digits=5)),rbind(round(forward.glm$deviance, digits=5),round(back.glm$deviance, digits=5),round(step.glm$deviance, digits=5))))
colnames(model_stats) <- c("Selection Type", "AIC", "Residual Deviance")
kable(model_stats,format="markdown", booktabs = T)
model_stats <- data.frame(cbind(rbind("Forwards","Backwards","Stepwise"),rbind(round(forward.glm$aic, digits=2),round(back.glm$aic, digits=2),round(step.glm$aic, digits=2)),rbind(round(forward.glm$deviance, digits=2),round(back.glm$deviance, digits=2),round(step.glm$deviance, digits=2))))
colnames(model_stats) <- c("Selection Type", "AIC", "Residual Deviance")
kable(model_stats,format="markdown", booktabs = T)
dfTrain$action_type = as.character(dfTrain$action_type)
dfTrain = dfTrain %>%
mutate(action_type = if_else(action_type == "Running Tip Shot", 'Tip Shot', action_type))%>%
mutate(action_type = if_else(action_type == "Tip Layup Shot", 'Tip Shot', action_type)) %>%
mutate(action_type = if_else(action_type == "Putback Slam Dunk Shot","Slam Dunk Shot",action_type)) %>%
mutate(action_type = if_else(action_type == "Running Slam Dunk Shot","Slam Dunk Shot",action_type))
dfTrain$action_type = as.factor(dfTrain$action_type)
df.preds$action_type = as.character(df.preds$action_type)
df.preds = df.preds %>%
mutate(action_type = if_else(action_type == "Turnaround Finger Roll Shot", 'Finger Roll Shot', action_type)) %>%
mutate(action_type = if_else(action_type == "Putback Slam Dunk Shot",'Slam Dunk Shot',action_type)) %>%
mutate(action_type = if_else(action_type == "Running Slam Dunk Shot",'Slam Dunk Shot',action_type))
df.preds$action_type = as.factor(df.preds$action_type)
#K-fold CV
set.seed(100)
Train <- createDataPartition(dfTrain$shot_made_flag, p=0.75, list=FALSE)
training <- dfTrain[ Train, ]
testing <- dfTrain[ -Train, ]
#train for specficity??? option
ctrl <- trainControl(method = "repeatedcv",
number = 25,
repeats = 5,
classProbs = T)
#combined shot type used instead of action type - test set has action types that are not in the training set
mod_fit <- train(shot_made_flag ~ action_type + attendance + arena_temp +
game_event_id + seconds_remaining + minutes_remaining +
loc_y + game_date + loc_x + playoffs,
data=training, method="glm",
family="binomial",
trControl = ctrl,
tuneLength = 5,
metric = "logLoss")
pred = predict(mod_fit, newdata=testing)
cf = confusionMatrix(table(data=as.numeric(pred>0.5), testing$shot_made_flag))
misclassificationRateInternal = (cf$table[2,1]+cf$table[1,2]) / sum(cf$table)
#ROC/AUC
# Compute AUC for predicting Class with the model
pred <- prediction(pred, testing$shot_made_flag)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
aucInternal <- performance(pred, measure = "auc")
aucInternal <- aucInternal@y.values[[1]]
#LOG LOSS AND PREDICTION FOR TRAINING DATA
testing$prob = predict(mod_fit, newdata=testing)
loglossTraining = testing %>%
mutate(logloss = testing$shot_made_flag * log(1-testing$prob) + (1-testing$shot_made_flag)*log(1-testing$prob))
#Will generate log loss value
loglossValue = -1/5174 * sum(loglossTraining$logloss)
########################################################
#External Model performance Metrics
ex.pred = predict(mod_fit, newdata=dfTrain)
ex.cf = confusionMatrix(table(data=as.numeric(ex.pred>0.5), dfTrain$shot_made_flag))
misclassificationRateEx = (cf$table[2,1]+cf$table[1,2]) / sum(cf$table)
#ROC/AUC
# Compute AUC for predicting Class with the model
expredROC <- prediction(ex.pred, dfTrain$shot_made_flag)
experf <- performance(expredROC, measure = "tpr", x.measure = "fpr")
aucEx <- performance(expredROC, measure = "auc")
aucEx <- aucEx@y.values[[1]]
#LOG LOSS AND PREDICTION FOR External CV
dfTrain$prob = predict(mod_fit, newdata=dfTrain)
loglossTraining = dfTrain %>%
mutate(logloss = dfTrain$shot_made_flag * log(1-dfTrain$prob) + (1-dfTrain$shot_made_flag)*log(1-dfTrain$prob))
#Will generate log loss value
loglossValue = -1/20697 * sum(loglossTraining$logloss)
lr.SpecSense.confusion.internal <- data.frame(cf$byClass)
lr.AccuracyP.confusion.internal <- data.frame(cf$overall)
lr.Accuracy.confusion.internal <- AccuracyP.confusion.internal[1,]
lr.Sensitivity.confusion.internal <- SpecSense.confusion.internal[1,]
lr.Specificity.confusion.internal <- SpecSense.confusion.internal[2,]
lr.Precision.confusion.internal <- SpecSense.confusion.internal[5,]
######### External Cross-Validation Metrics
lr.SpecSense.confusion.external <- data.frame(ex.cf$byClass)
lr.AccuracyP.confusion.external <- data.frame(ex.cf$overall)
lr.Accuracy.confusion.external <- lr.AccuracyP.confusion.external[1,]
lr.Sensitivity.confusion.external <- lr.SpecSense.confusion.external[1,]
lr.Specificity.confusion.external <- lr.SpecSense.confusion.external[2,]
lr.Precision.confusion.external <- lr.SpecSense.confusion.external[5,]
### Internal Cross-Validation Confusion Matrix
lr.confusionFrame.internal <- data.frame(rbind(round(lr.Sensitivity.confusion.internal, digits=5),round(lr.Specificity.confusion.internal, digits=5),round(lr.Precision.confusion.internal, digits=5),round(lr.Accuracy.confusion.internal, digits=5),round(misclassificationRateInternal, digits=5),round(loglossValue, digits=5),round(aucInternal, digits=5)))
rownames(lr.confusionFrame.internal) <- c("Sensitivity","Specificity","Precision","Accuracy","Misclassification Rate","Logarithmic Loss","Area Under the Curve")
### External Cross-Validation Confusion Matrix
lr.confusionFrame.external <- data.frame(rbind(round(lr.Sensitivity.confusion.external, digits=5),round(lr.Specificity.confusion.external, digits=5),round(lr.Precision.confusion.external, digits=5),round(lr.Accuracy.confusion.external, digits=5),round(misclassificationRateEx, digits=5),round(loglossValue, digits=5),round(AUC.external, digits=5)))
rownames(lr.confusionFrame.external) <- c("Sensitivity","Specificity","Precision","Accuracy","Misclassification Rate","Logarithmic Loss","Area Under the Curve")
lr.confusionFrame <- data.frame(lr.confusionFrame.internal, lr.confusionFrame.external)
colnames(lr.confusionFrame) <- c("Internal CV Statistics", "External CV Statistics")
suppressWarnings(kable(lr.confusionFrame, format="markdown", booktabs = T))  %>%
kable_styling(latex_options="striped", position = "center")
topshots = as.data.frame(coef(mod_fit$finalMode))
names(topshots)[1] = "Coefficient (logit)"
topshots$`Odds Ratio` = exp(topshots$`Coefficient (logit)`)
topshots =  topshots[order(topshots$`Odds Ratio`),]
#Predicitons
df.preds$prob = predict(mod_fit, newdata=df.preds)
logistic_prediction =  df.preds %>%select(recId, prob)
suppressWarnings(kable(head(topshots),
format="markdown", booktabs = T)) %>%
kable_styling(latex_options="striped", position = "center")
topshots.Top6 <- head(topshots)
dim(topshots.Top6)
topshots.Top6
topshots.Top6$`Coefficient (logit)`
data.frame(head(topshots))
topshots.Top6 <- data.frame(head(topshots))
topshots.Top6$Coefficient..logit.
corrplot::corrplot(cor(df.numeric %>% subset(select=-c(shot_made_flag)))
, title = "Correlation among Predictor Variables"
, type = "lower"
, tl.pos = "ld"
, method = "square"
, tl.cex = 0.65
, tl.col = 'red'
, order = "alphabet"
, diag = F
, mar=c(0,0,5,0)
, bg="ivory1"
,tl.srt=.05
)
kable(collinear.correlation_Ten, format="latex", booktabs = T)
kable(bartlettsTest,
format="latex", booktabs = T)
par(mfrow=c(2,2))
### Internal Cross-Validation
plot(perf_step.internal, main = "QDA ROC Curve - Internal CV")
### External Cross-Validation
plot(perf_step.external, main = "QDA ROC Curve - External CV")
plot(perf, main = " Logisitic Regression
ROC Curve - Internal CV")
plot(experf, main = "Logisitic Regression
ROC Curve - External CV")
par(mfrow=c(1,1))
modecoef = as.data.frame(coef(mod_fit$finalMode))
names(modecoef)[1] = "Coefficient (logit)"
modecoef$`Odds Ratio` = exp(modecoef$`Coefficient (logit)`)
suppressWarnings(kable(modecoef, format="latex", booktabs = T, longtable = TRUE)) %>%
kable_styling(latex_options = c("repeat_header"), font_size = 10)
suppressWarnings(kable(modecoef, format="latex", booktabs = T, longtable = TRUE)) %>%
kable_styling(latex_options = c("repeat_header", "striped"), position = "center", font_size = 10)
action_type_ft = as.data.frame(table(dfTrain$action_type))
suppressWarnings(kable(action_type_ft, format="latex", booktabs = T, longtable = TRUE)) %>%
kable_styling(latex_options = c("repeat_header", "striped"), position = "center", font_size = 10)
kable(bartlettsTest,
format="latex", booktabs = T)
library(pacman)
p_load(rrcov, MASS, dplyr, purrr, ggplot2, Hmisc, pcaPP, knitr, kableExtra, caret, cluster, robustbase, ROCR, Metrics, bookdown)
# first, install MikTex, then run the R command install_tinytex() in order to generate the PDF from the LaTeX markdown.
df <- read.csv("./modelingKobeData.csv", header=T, sep=",", strip.white=T, stringsAsFactors = F, na.strings=c(""))
df.preds <- read.csv("./predictionKobeData.csv", header=T, sep=",", strip.white=T, stringsAsFactors = F, na.strings=c(""))
shotsTaken <- data.frame(df$loc_x, df$loc_y, df$shot_distance)
colnames(shotsTaken) <- c("loc_x", "loc_y", "shot_distance")
# simple plot using EVENT_TYPE to colour the dots
ggplot(shotsTaken, aes(x=loc_x, y=loc_y)) +
geom_point(aes(colour = df$shot_type))
df[which(df$loc_y > 300),"shot_type"] <- "3PT Field Goal"
df.preds[which(df.preds$loc_y > 300),"shot_type"] <- "3PT Field Goal"
# Convert the points to integer values since they have integer value in reality
df$shot_type <- ifelse(df$shot_type=="2PT Field Goal", 2, 3)
df.preds$shot_type <- ifelse(df.preds$shot_type=="2PT Field Goal", 2, 3)
tryCatch(
{
# Convert all integers to numeric and characters to factors with levels:
df <- df %>% mutate_if(is.integer, as.numeric) %>% mutate_if(is.character, as.factor) %>% data.frame()
df <- df %>%
subset(select=-c(team_id, # dropping since this is a uniform distribution of data
team_name, # dropping since this is a uniform distribution of data. Also collinear with team_id
combined_shot_type, # dropping this in favor of action_type
shot_zone_area, # this is ambiguous and less descriptive than geospatial data
matchup # removing in favor of opponent; Kobe only played for LAL so that will never change
)
)
# create numeric dataframe for correlation plot
df.numeric <- df %>% keep(is.numeric)
df.preds <- df.preds %>% mutate_if(is.integer, as.numeric) %>% mutate_if(is.character, as.factor) %>% data.frame()
df.preds <- df.preds %>%
subset(select=-c(team_id, # dropping since this is a uniform distribution of data
team_name, # dropping since this is a uniform distribution of data. Also collinear with team_id
combined_shot_type, # dropping this in favor of action_type
shot_zone_area, # this is ambiguous and less descriptive than geospatial data
matchup # removing in favor of opponent; Kobe only played for LAL so that will never change
)
)
# create numeric dataframe for correlation plot
df.numeric.preds <- df.preds %>% keep(is.numeric)
},
error = function(e)
{
e
}
)
corr.plot <- corrplot::corrplot(cor(df.numeric %>% subset(select=-c(shot_made_flag)))
, title = "Correlation among Predictor Variables"
, type = "lower"
, tl.pos = "ld"
, method = "square"
, tl.cex = 0.65
, tl.col = 'red'
, order = "alphabet"
, diag = F
, mar=c(0,0,5,0)
, bg="ivory1"
,tl.srt=.05
)
df <- df %>% subset(select=-c(lat, # dropping lat because it is collinear with loc_y and shot_distance
lon, # dropping lon because it is collinear with loc_x and shot_distance
period, # dropping period in favor of game event id because game event id is more descriptive and continuous
game_id # dropping playoffs for game_id; game ID can capture playoffs seasonally
)
)
df.preds <- df.preds %>% subset(select=-c(lat, # dropping lat because it is collinear with loc_y and shot_distance
lon, # dropping lon because it is collinear with loc_x and shot_distance
period, # dropping period in favor of game event id because game event id is more descriptive and continuous
game_id # dropping playoffs for game_id; game ID can capture playoffs seasonally
)
)
df.numeric <- df %>% keep(is.numeric) %>% mutate_if(is.integer, as.numeric)
df.numeric.preds <- df.preds %>% keep(is.numeric) %>% mutate_if(is.integer, as.numeric)
flattenCorrMatrix <- function(cormatrix, pmatrix) {
ut <- upper.tri(cormatrix)
data.frame(
row = rownames(cormatrix)[row(cormatrix)[ut]],
column = rownames(cormatrix)[col(cormatrix)[ut]],
cor  =(cormatrix)[ut],
p = pmatrix[ut]
)
}
options(scipen=999)
options(max.print=100000)
#See what variables are correlated with eachother, p-values
correlation.matrix <- Hmisc::rcorr(as.matrix(df.numeric), type="pearson")
corDF <- data.frame(flattenCorrMatrix(correlation.matrix$r, correlation.matrix$P))
corDF.ordered <- data.frame(corDF[order(-corDF$cor),])
cordDF.ordered.Top10 <- data.frame(head(corDF.ordered, 10))
colnames(cordDF.ordered.Top10) = c("Correlation Predictor Variable", "Correlation Response Variable", "Correlation", "p-Value")
cordDF.ordered.Top10$Correlation <- round(as.numeric(as.character(cordDF.ordered.Top10$Correlation)), digits=5)
cordDF.ordered.Top10$`p-Value` <- ifelse(as.numeric(as.character(cordDF.ordered.Top10$`p-Value`)) < 0.0001, "p < 0.0001", as.numeric(as.character(cordDF.ordered.Top10$`p-Value`)))
dfTrain.numeric <- df.numeric[which(!is.na(df.numeric$shot_made_flag)),]
prediction.Data.numeric <- df.numeric[which(is.na(df.numeric$shot_made_flag)),]
dfTrain.numeric$shot_made_flag <- as.factor(dfTrain.numeric$shot_made_flag)
dfTrain.numeric$shot_made_flag <- ifelse(dfTrain.numeric$shot_made_flag=="1", "made", "not_made")
dfTrain.numeric <- dfTrain.numeric %>% mutate_if(is.integer, as.numeric) %>% mutate_if(is.character, as.factor) %>% data.frame()
Bartlett_ChiSq <- rrcov::Wilks.test(shot_made_flag ~ ., data=dfTrain.numeric, method = "c", approximation = "Bartlett")
# Wilk's Lambda produces significant p-value in Bartlett's test so we need to use a Quadratic Discriminant Analysis instead of Linear
format(round(Bartlett_ChiSq$p.value, 2), nsmall=4)
# Wilks' Lambda plus degrees of freedom used in Bartlett's chi-squared test
WilksDegreesofFreedom <- rbind(as.numeric(paste0(Bartlett_ChiSq$parameter, sep = " ")))
# p-value from Bartlett's test
Bartlett_ChiSq$p.value
Bartletts_p <- format(round(as.numeric(Bartlett_ChiSq$p.value), 2), nsmall=4)
# Because Bartlett's p-value is less than 0.0001 (indicated above), updating to shorter form:
Bartletts_p = ifelse(Bartlett_ChiSq$p.value < 0.0001, "p < 0.0001", Bartlett_ChiSq$p.value)
#Bartletts_p <- "p < 0.0001"
dfBartlett <- data.frame(WilksDegreesofFreedom, Bartlett_ChiSq$wilks, Bartletts_p)
colnames(dfBartlett) <- c("Chi-Square Statistic", "Degrees of Freedom", "Wilks' Lambda", "p-Value")
dfBartlett$`Chi-Square Statistic` <- round(as.numeric(as.character(dfBartlett$`Chi-Square Statistic`)), digits=5)
dfBartlett$`Wilks' Lambda` <- round(as.numeric(as.character(dfBartlett$`Wilks' Lambda`)), digits=5)
bartlettsTest <- data.frame(rbind(dfBartlett$`Chi-Square Statistic`,dfBartlett$`Degrees of Freedom`,dfBartlett$`Wilks' Lambda`,Bartletts_p))
rownames(bartlettsTest) <- c("Chi Square Statistic","Degrees of Freedom","Wilks' Lambda","p-Value")
colnames(bartlettsTest) <- "Statistics"
dfTrain <- df[which(!is.na(df$shot_made_flag)),]
prediction.Data <- df[which(is.na(df$shot_made_flag)),]
### Full Data train/test split for Logistic
test_sample_size <- floor(0.75 * nrow(dfTrain))
set.seed(123)
train_ind <- sample(seq_len(nrow(dfTrain)), size = test_sample_size)
subDF.Train <- dfTrain[train_ind, ] #75% training
subDF.Test <- dfTrain[-train_ind, ] # 25% testing
#### Numeric Data train/test split for QDA
test_sample_size <- floor(0.75 * nrow(dfTrain.numeric))
set.seed(123)
train_ind <- sample(seq_len(nrow(dfTrain.numeric)), size = test_sample_size)
subDF.Train.numeric <- dfTrain.numeric[train_ind, ] #75% training
subDF.Test.numeric <- dfTrain.numeric[-train_ind, ] # 25% testing
kobe.qda <- qda(shot_made_flag ~ ., CV=T, data=dfTrain.numeric)
data.frame(mean(kobe.qda$posterior[,1]), mean(kobe.qda$posterior[,2]))
shot_made_flag_Posterior <- rbind("0", "1")
proportion_Posterior <- rbind(mean(kobe.qda$posterior[,1]), mean(kobe.qda$posterior[,2]))
priori <- data.frame(shot_made_flag_Posterior, proportion_Posterior)
subDF.Train.numeric$shot_made_flag <- as.factor(subDF.Train.numeric$shot_made_flag)
#subDF.Train.numeric$shot_made_flag <- ifelse(subDF.Train.numeric$shot_made_flag=="1", "made", "not_made")
subDF.Train.numeric <- subDF.Train.numeric %>% mutate_if(is.integer, as.numeric) %>% mutate_if(is.character, as.factor) %>% data.frame()
# k=25 folds, repeat random folding for internal cross-validation 5 times:
train.Control <- caret::trainControl(method = "repeatedcv",
number = 25,
repeats = 5,
#summaryFunction = twoClassSummary,
summaryFunction = mnLogLoss,
classProbs = T) # classProbs=T to get mnLogLoss (also for twoClassSummary)
# build the model using the 75% partitioned from the internal dataset (the set with all shot_made_flag response results):
qda.filtered <- train(shot_made_flag ~ .
, data = subDF.Train.numeric
, method = "qda"
, trControl=train.Control
, preProcess = c("center", "scale", "spatialSign")
#, preProcess = "spatialSign"
, metric = "logLoss"
)
# test the model on the 25% partitions from the internal dataset:
internal_cv.predicted.qda <- suppressWarnings(predict(qda.filtered, newdata = subDF.Test.numeric))
# build a confusion matrix for internal cross-validation to see performance:
confusion_matrix_results.internal<- confusionMatrix(table(internal_cv.predicted.qda, subDF.Test.numeric$shot_made_flag))
external_cv.predicted.qda <- suppressWarnings(predict(qda.filtered, newdata = dfTrain.numeric))
# compare the predicted results to the actual results to make sure model still performs as intended:
confusion_matrix_results.external <- confusionMatrix(table(external_cv.predicted.qda, dfTrain.numeric$shot_made_flag))
class(hook_output)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
# this hook is used only when the linewidth option is not NULL
if (!is.null(n <- options$linewidth)) {
x = knitr:::split_lines(x)
# any lines wider than n should be wrapped
if (any(nchar(x) > n)) x = strwrap(x, width = n)
x = paste(x, collapse = '\n')
}
hook_output(x, options)
})
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
# this hook is used only when the linewidth option is not NULL
if (!is.null(n <- options$linewidth)) {
x = knitr:::split_lines(x)
# any lines wider than n should be wrapped
if (any(nchar(x) > n)) x = strwrap(x, width = n)
x = paste(x, collapse = '\n')
}
hook_output(x, options)
})
class(hook_output)
?hook_output
library(pacman)
p_load(rrcov, MASS, dplyr, purrr, ggplot2, Hmisc, pcaPP, knitr, kableExtra, caret, cluster, robustbase, ROCR, Metrics, bookdown, ResourceSelection, usdm)
