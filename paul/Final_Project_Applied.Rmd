---
title: "Will Kobe Bryant Make His Next Shot:  Quadratic Discriminant Analysis and Logistic Regression using R"
author:
- Paul Adams
- Reannan McDaniel
- Jeff Nguyen
date: |
  Master of Science in Data Science, Southern Methodist University, USA
lang: en-US
class: man
figsintext: true
numbersections: true
encoding: UTF-8
bibliography: references.bib
biblio-style: apalike
output:
  bookdown::pdf_document2:
     citation_package: natbib
     keep_tex: true
     toc: true
#header-includes:
#   - \usepackage{amsmath}
#   - \usepackage[utf8]{inputenc}
#   - \usepackage[T1]{fontenc}
#   - \usepackage{setspace}
#   - \usepackage{hyperref}
#   - \onehalfspacing
editor_options: 
  chunk_output_type: console
  
---

```{r Setup and Loading Packages, echo=F, include=F, warning=F}

library(pacman)
p_load(rrcov, MASS, dplyr, purrr, ggplot2, Hmisc, pcaPP, knitr, kableExtra, caret, cluster, robustbase, ROCR, Metrics)
# first, install MikTex, then run the R command install_tinytex() in order to generate the PDF from the LaTeX markdown.

df <- read.csv("./modelingKobeData.csv", header=T, sep=",", strip.white=T, stringsAsFactors = F, na.strings=c(""))

df.preds <- read.csv("./predictionKobeData.csv", header=T, sep=",", strip.white=T, stringsAsFactors = F, na.strings=c(""))

sapply(df, function(cnt) sum(length(which(is.na(cnt)))))

```

# **Abstract:**  
### *This project investigates the correlation between multiple potential explanatory variables and Kobe Bryant's ability to make a shot while playing for the NBA team Los Angeles Lakers using data gathered from `1996`-`2015`.*

# **Introduction**
### This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. This is a sample introduction. Nothing but a sample. But a necessary sample to preserve space. 
# **Exploratory Data Analysis**

## **Outlier Check**
### First, we performed a brief outlier check, which included a graphical analysis of all shots taken, by loc_x and loc_y. This graphical analysis indicated a 2PT (2-point) Field Goal was recorded from the 3PT (3-point) range. Upon inspection of other attributes - such as action type and shot_zone_range - we verified this shot to be a member member of the 3-point level of shot_type. Under the assumption shots from beyond the 300 inch mark are more likely to have been incorrectly recorded as 2 points rather than an incorrectly recorded location y, we modified our programming to transform all shots where $loc_y > 300$ to be recoded as `3PT Field Gloal`.

```{r outlier check, echo=F, include= F, warning=F}

# unlist shot data, save into a data frame
shotsTaken <- data.frame(df$loc_x, df$loc_y, df$shot_distance)

colnames(shotsTaken) <- c("loc_x", "loc_y", "shot_distance")

# simple plot using EVENT_TYPE to colour the dots
ggplot(shotsTaken, aes(x=loc_x, y=loc_y)) + 
  geom_point(aes(colour = df$shot_type))

df[which(df$loc_y > 300),"shot_type"] <- "3PT Field Goal"
df.preds[which(df.preds$loc_y > 300),"shot_type"] <- "3PT Field Goal"

# Convert the points to integer values since they have integer value in reality
df$shot_type <- ifelse(df$shot_type=="2PT Field Goal", 2, 3)
df.preds$shot_type <- ifelse(df.preds$shot_type=="2PT Field Goal", 2, 3)

```

## **Variable Elimination**
### Next, we removed one-level factors. These will never change so are not useful to the model; including can cause issues with model sensitivity since linear trajectories will be down-weighted. Therefore, their significance will be lessened by the constant state of the additional parameters. While this is may not be significant, it is not condusive to model quality.

```{r Transformations Phase Two, echo=F, warning=F}

tryCatch(
    {
    # Convert all integers to numeric and characters to factors with levels:
    df <- df %>% mutate_if(is.integer, as.numeric) %>% mutate_if(is.character, as.factor) %>% data.frame()
    df <- df %>% 
      subset(select=-c(team_id, # dropping since this is a uniform distribution of data
                       team_name, # dropping since this is a uniform distribution of data. Also collinear with team_id
                       action_type, # dropping this in favor of combined_shot_type
                       shot_zone_area, # this is ambiguous and less descriptive than geospatial data
                       shot_zone_basic, # this is ambiguous and less descriptive than geospatial data
                       shot_zone_range, # this is ambiguous and less descriptive than geospatial data
                       matchup # removing in favor of opponent; Kobe only played for LAL so that will never change
                      )
              )
    # create numeric dataframe for correlation plot
    df.numeric <- df %>% keep(is.numeric)
    
    df.preds <- df.preds %>% mutate_if(is.integer, as.numeric) %>% mutate_if(is.character, as.factor) %>% data.frame()
    df.preds <- df.preds %>% 
      subset(select=-c(team_id, # dropping since this is a uniform distribution of data
                       team_name, # dropping since this is a uniform distribution of data. Also collinear with team_id
                       action_type, # dropping this in favor of combined_shot_type
                       shot_zone_area, # this is ambiguous and less descriptive than geospatial data
                       shot_zone_basic, # this is ambiguous and less descriptive than geospatial data
                       shot_zone_range, # this is ambiguous and less descriptive than geospatial data
                       matchup # removing in favor of opponent; Kobe only played for LAL so that will never change
                      )
              )
    # create numeric dataframe for correlation plot
    df.numeric.preds <- df.preds %>% keep(is.numeric)
    },
    error = function(e)
    {
      e
    }
)

```



## **Addressing Multicollinearity: Correlation Plot for Visual Data Exploration**
### To address multicollinearity among quantitative predictor variables, a correlation heat map was created for visual inspection of correlation. Please see Appendix A to view this correlation heat map.
```{r Multicollinearity, echo=F, include=F, warning=F}

corr.plot <- corrplot::corrplot(cor(df.numeric %>% subset(select=-c(shot_made_flag)))
                   , title = "Correlation among Predictor Variables"
                   , type = "lower"
                   , tl.pos = "ld"
                   , method = "square"
                   , tl.cex = 0.65
                   , tl.col = 'red'
                   , order = "alphabet"
                   , diag = F
                   , mar=c(0,0,5,0)
                   , bg="ivory1"
                   ,tl.srt=.05
)

```
\pagebreak
## **Post-Correlation Heat Map Variable Elimination**
### Following our correlation heat map, we decided to eliminate some collinear terms. However, some of the collinearity is useful to capture the instances where the terms are unique. For example, `combined_shot_type` (factor variable) is collinear with `shot_distance` (quantitative variable), but it also accounts for the method Kobe may use to make a shot. For example, distance may be relatively the same between 10 and 11 feet, but the factor levels used to derrive their `short` or `far` indications may differ. This difference could be whether Kobe makes a potentially more accurate heel-planted shot or if he is forced to lean forward and take a riskier shot at basket; the difference in distance may only be one foot, but the difference in technique could measure significant relative to the odds of success.

```{r Post-Correlation Plot Variable Elimination, echo=F, warning=F}

df <- df %>% subset(select=-c(lat, # dropping lat because it is collinear with loc_y and shot_distance
                              lon, # dropping lon because it is collinear with loc_x and shot_distance
                              period, # dropping period in favor of game event id because game event id is more descriptive and continuous
                              game_id # dropping playoffs for game_id; game ID can capture playoffs seasonally
                             )
                    )

df.preds <- df.preds %>% subset(select=-c(lat, # dropping lat because it is collinear with loc_y and shot_distance
                              lon, # dropping lon because it is collinear with loc_x and shot_distance
                              period, # dropping period in favor of game event id because game event id is more descriptive and continuous
                              game_id # dropping playoffs for game_id; game ID can capture playoffs seasonally
                             )
                    )

df.numeric <- df %>% keep(is.numeric) %>% mutate_if(is.integer, as.numeric)
df.numeric.preds <- df.preds %>% keep(is.numeric) %>% mutate_if(is.integer, as.numeric)

```

## **Addressing Multicollinearity: Correlation Matrix for Numerical Analysis**
### After deselecting the most obvious collinear terms through visually inspection of the correlation plot, a correlation matrix for analyzing the remaining results. Collinear quantitative data was preliminarily removed following correlation plot analysis to desaturate the model to an extent that allows more distinction among significance measures for terms in the correlation matrix.

```{r Correlation Matrix for Quantitative Data, echo=F, include=F, warning=F}
flattenCorrMatrix <- function(cormatrix, pmatrix) {
  ut <- upper.tri(cormatrix)
  data.frame(
    row = rownames(cormatrix)[row(cormatrix)[ut]],
    column = rownames(cormatrix)[col(cormatrix)[ut]],
    cor  =(cormatrix)[ut],
    p = pmatrix[ut]
  )
}

options(scipen=999)
options(max.print=100000)

#See what variables are correlated with eachother, p-values
correlation.matrix <- Hmisc::rcorr(as.matrix(df.numeric), type="pearson")
corDF <- data.frame(flattenCorrMatrix(correlation.matrix$r, correlation.matrix$P))

corDF.ordered <- data.frame(corDF[order(-corDF$cor),])
collinear.correlation <- corDF[which(corDF$cor >= 0.50),]

collinear.correlation <- table(collinear.correlation[order(-collinear.correlation$cor),])
collinear.correlation_Ten <- head(data.frame(collinear.correlation), 10)
collinear.correlation_Ten <- collinear.correlation_Ten[,1:4]
colnames(collinear.correlation_Ten) = c("Correlation Predictor Variable", "Correlation Response Variable", "Correlation", "p-Value")

collinear.correlation_Ten$Correlation <- round(as.numeric(as.character(collinear.correlation_Ten$Correlation)), digits=5)

collinear.correlation_Ten$`p-Value` <- ifelse(as.numeric(as.character(collinear.correlation_Ten$`p-Value`)) < 0.0001, "p < 0.0001", as.numeric(as.character(collinear.correlation_Ten$`p-Value`)))

#########
#write.csv(colinear.correlation, "Collinear_Correlation_Matrix.csv")
#write.csv(corDF.ordered, "All_Vars_Correlation_Matrix.csv")

```

```{r Correlation Matrix Table, echo=F, include=F, warning=F, fig.cap = "Top 10 Collinear Terms", size="small"}

kable(collinear.correlation_Ten, format="latex", booktabs = T)  %>%
  kable_styling(latex_options="hold_position", position = "center")

```

# **Quadratic Discriminant Analysis**

### As requested within the requirements of this study, a Linear Discriminant Analysis must be assessed and provided. Discriminant analysis is an operation that compares a categorical response variable against measures of quantitative predictor variables. As a result, analysis for this section is performed on the numerical predictors, which include `recId`, `game_event_id`, `game_id`, `loc_x`, `loc_y`, `minutes_remaining`, `seconds_remaining`, `shot_distance`, `shot_made_flag`, `shot_type`, `game_date`, `shot_id`, `attendance`, `arena_temp`, `avgnoisedb`, controlling collinearity by eliminating a member of each collinear pair prior to model development.

### `Linear Discriminant Analysis` requires a linear boundary between the predictor variables, respective of the response. If the boundary between predictors and response is not linear, `Quadratic Discriminant Analysis` (QDA) must be used. `Wilks' Lambda` distribution is used to assess the nature of boundary linearity, which is a required understanding to develop a well-fit discriminant classification model. However, because of the large dimensions of the data set analyzed in this study, an approximation of Wilks' Lambda must be used, rather than Wilks' Lambda itself. `Bartlett's Test` is an approximation of Wilks' Lambda that can be used for models with large dimensions by applying a measure against the `Chi-Square distribution`. This method is applied herein to assess linearity.
\pagebreak

## **Bartlett's Test**
### The result of the Bartlett's test returned statistically significant results, indicating the null hypothesis of linearity must be rejected in favor of the alternate, which is that the discriminant boundary is non-linear. Consequently, we proceed with a model based on `Quadratic Discriminant Analysis` to provide predictive responses from a discriminant model. However, we proceed with caution, as the quadratic version of the discriminant analysis is at greater risk for over-fitting to the data than Linear Discriminant Analysis as the boundary is required to conform more closely to the data rather than to the mean of the data. This was also taken into consideration when assessing the results of the Logistic Regression model development that occurs afterward.  Bartlett's Test of this data set yielded a significant p-value, where p < 0.0001, indicating that the proportion of distribution beyond the derrived test statistic is beyond that which could be explained by chance. Therefore, we must reject the null hypothesis that the boundary for analysis is linear; the boundary is non-linear. Thus, an analysis using Quadratic Discriminant Analysis is applied.

```{r Quadratic Discriminant Analysis: Test Identification, echo=F, include=F, warning=F}

dfTrain.numeric <- df.numeric[which(!is.na(df.numeric$shot_made_flag)),]
prediction.Data.numeric <- df.numeric[which(is.na(df.numeric$shot_made_flag)),]

dfTrain.numeric$shot_made_flag <- as.factor(dfTrain.numeric$shot_made_flag)
dfTrain.numeric$shot_made_flag <- ifelse(dfTrain.numeric$shot_made_flag=="1", "made", "not_made")
dfTrain.numeric <- dfTrain.numeric %>% mutate_if(is.integer, as.numeric) %>% mutate_if(is.character, as.factor) %>% data.frame()

Bartlett_ChiSq <- rrcov::Wilks.test(shot_made_flag ~ ., data=dfTrain.numeric, method = "c", approximation = "Bartlett")

# Wilk's Lambda produces significant p-value in Bartlett's test so we need to use a Quadratic Discriminant Analysis instead of Linear
format(round(Bartlett_ChiSq$p.value, 2), nsmall=4)

# Wilks' Lambda plus degrees of freedom used in Bartlett's chi-squared test
WilksDegreesofFreedom <- rbind(as.numeric(paste0(Bartlett_ChiSq$parameter, sep = " ")))

# p-value from Bartlett's test
Bartlett_ChiSq$p.value
Bartletts_p <- format(round(as.numeric(Bartlett_ChiSq$p.value), 2), nsmall=4)

# Because Bartlett's p-value is less than 0.0001 (indicated above), updating to shorter form:
Bartletts_p = ifelse(Bartlett_ChiSq$p.value < 0.0001, "p < 0.0001", Bartlett_ChiSq$p.value)
#Bartletts_p <- "p < 0.0001"

dfBartlett <- data.frame(WilksDegreesofFreedom, Bartlett_ChiSq$wilks, Bartletts_p)
colnames(dfBartlett) <- c("Chi-Square Statistic", "Degrees of Freedom", "Wilks' Lambda", "p-Value")

dfBartlett$`Chi-Square Statistic` <- round(as.numeric(as.character(dfBartlett$`Chi-Square Statistic`)), digits=5)
dfBartlett$`Wilks' Lambda` <- round(as.numeric(as.character(dfBartlett$`Wilks' Lambda`)), digits=5)

bartlettsTest <- data.frame(rbind(dfBartlett$`Chi-Square Statistic`,dfBartlett$`Degrees of Freedom`,dfBartlett$`Wilks' Lambda`,Bartletts_p))
rownames(bartlettsTest) <- c("Chi Square Statistic","Degrees of Freedom","Wilks' Lambda","p-Value")
colnames(bartlettsTest) <- "Metric Output"
```

```{r Bartletts Results, echo=F, include=F, warning=F}

kable(bartlettsTest,
      format="latex", caption = "Bartlett Test's Wilks' Lambda Approximation", booktabs = T)  %>%
  kable_styling(position = "center")

```

### Following the removal of predictor variables after visually inspecting the correlation heat map, we analyzed a correlation matrix. However, the matrix itself did not identify any remaining collinearity at a threshold of correlation necessitating removal of like-terms. Consequently, no further predictor variables are removed. Therefore, modeling data is broken into a 75% training / 25% testing data split for internal cross-cross validation. The objective of internal cross-validation is to develop a model using 75% of the data and test it on the remaining 25% in order to assess model fit statistics. Typically, following internal cross-validation, external cross-validation is performed.

```{r Make Train and Test Data, echo=F, include=F, warning=F}

dfTrain <- df[which(!is.na(df$shot_made_flag)),]
prediction.Data <- df[which(is.na(df$shot_made_flag)),]

### Full Data train/test split for Logistic
test_sample_size <- floor(0.75 * nrow(dfTrain))
set.seed(123)
train_ind <- sample(seq_len(nrow(dfTrain)), size = test_sample_size)
subDF.Train <- dfTrain[train_ind, ] #75% training
subDF.Test <- dfTrain[-train_ind, ] # 25% testing

#### Numeric Data train/test split for QDA
test_sample_size <- floor(0.75 * nrow(dfTrain.numeric))
set.seed(123)
train_ind <- sample(seq_len(nrow(dfTrain.numeric)), size = test_sample_size)
subDF.Train.numeric <- dfTrain.numeric[train_ind, ] #75% training
subDF.Test.numeric <- dfTrain.numeric[-train_ind, ] # 25% testing
```

```{r Quadratic Discriminant Analysis: Test Production, echo=F, warning=F, include=F}
##################################################################################################
##################################################################################################
##################################################################################################
                                    ####### a Priori #######
# MASS package used for qda()
#df.numeric <- df.numeric[order(df.numeric$shot_made_flag),]
kobe.qda <- qda(shot_made_flag ~ ., CV=T, data=dfTrain.numeric)

data.frame(mean(kobe.qda$posterior[,1]), mean(kobe.qda$posterior[,2]))

shot_made_flag_Posterior <- rbind("0", "1")
proportion_Posterior <- rbind(mean(kobe.qda$posterior[,1]), mean(kobe.qda$posterior[,2]))
priori <- data.frame(shot_made_flag_Posterior, proportion_Posterior)



##################################################################################################
##################################################################################################
##################################################################################################
```

# **Quadratic Discriminant Analysis: Internal Cross-Valdiation and Model Development**
### Following removal of significant levels of multicollinearity from the dataset and partitioning into a 75% training / 25% testing split, internal cross-validation is performed. The specifics of this test involves 25 folds of the data - meaning the 75% training data is divided into 25 partitions. The model is then trainied on 1/25th of the original 75%, then tested against the remaining 24/25ths, 1/25ths at-a-time. This test is repeated 5 times, with each repeat involving a different random partitioning of the 25 specified `folds` of the data. Finally, the model developed using the 75% training split is then applied to the 25% testing split and predictions are measured against the actuals of that split to develop model statistics such as `Accuracy`, `Misclassification`, `Precision`, `Sensitivity` and `Specificity`.

```{r Model Development & Internal Cross-Validation, echo=F, include=F, warning=F}

subDF.Train.numeric$shot_made_flag <- as.factor(subDF.Train.numeric$shot_made_flag)
#subDF.Train.numeric$shot_made_flag <- ifelse(subDF.Train.numeric$shot_made_flag=="1", "made", "not_made")
subDF.Train.numeric <- subDF.Train.numeric %>% mutate_if(is.integer, as.numeric) %>% mutate_if(is.character, as.factor) %>% data.frame()

# k=25 folds, repeat random folding for internal cross-validation 5 times:
train.Control <- caret::trainControl(method = "repeatedcv",
                              number = 25,
                              repeats = 5,
                              #summaryFunction = twoClassSummary,
                              summaryFunction = mnLogLoss,
                              classProbs = T) # classProbs=T to get mnLogLoss (also for twoClassSummary)

# build the model using the 75% partitioned from the internal dataset (the set with all shot_made_flag response results):
qda.filtered <- train(shot_made_flag ~ .
                , data = subDF.Train.numeric
                , method = "qda"
                , trControl=train.Control
                , preProcess = c("center", "scale", "spatialSign")
                #, preProcess = "spatialSign"
                , metric = "logLoss"
                 )

# test the model on the 25% partitions from the internal dataset:
internal_cv.predicted.qda <- suppressWarnings(predict(qda.filtered, newdata = subDF.Test.numeric))

# build a confusion matrix for internal cross-validation to see performance:
confusion_matrix_results.internal<- confusionMatrix(table(internal_cv.predicted.qda, subDF.Test.numeric$shot_made_flag))

```

# **Quadratic Discriminant Analysis: External Cross-Valdiation and Model Development**
### After building a model using internal cross-validation, which applied 5 repeated internal cross-validations across the 25 folds of training data, a confusion matrix was constructed and analyzed. Next, we applied the model developed using the 75% training split to make predictions against the entire portion of data that includes values for `shot_made_flag` in order to assess how closely the model can predict against the entire data set compared to the actuals. Applying the model to the entire dataset as `external cross-validation` provides the model an opportunity to test against different data and more closely simulate a real-life scenario than internal cross-validation. Internal and external cross-validation is performed for later Logistic Regression models as well. Following external cross-validation of both models, the metrics are compared to determine the best model (Quadratic Discriminant Analysis versus Logistic Regression).

```{r Predicting Internal Data for Model Metrics, echo=F, include=F, warning=F}

# apply the model developed above to the entire (75 + 25 = 100%) internal dataset. In other words, make predictions on the data that has results so they can be compared (no folds, no repeats this time):
external_cv.predicted.qda <- suppressWarnings(predict(qda.filtered, newdata = dfTrain.numeric))

# compare the predicted results to the actual results to make sure model still performs as intended:
confusion_matrix_results.external <- confusionMatrix(table(external_cv.predicted.qda, dfTrain.numeric$shot_made_flag))

```


```{r Gathering Specificity, Sensitivity, Accuracty from Confusion Matrix, echo=F, include=F, warning=F}

######### Internal Cross-Validation Metrics
SpecSense.confusion.internal <- data.frame(confusion_matrix_results.internal$byClass)
AccuracyP.confusion.internal <- data.frame(confusion_matrix_results.internal$overall)

Accuracy.confusion.internal <- AccuracyP.confusion.internal[1,]
Sensitivity.confusion.internal <- SpecSense.confusion.internal[1,]
Specificity.confusion.internal <- SpecSense.confusion.internal[2,]
Precision.confusion.internal <- SpecSense.confusion.internal[5,]

######### External Cross-Validation Metrics
SpecSense.confusion.external <- data.frame(confusion_matrix_results.external$byClass)
AccuracyP.confusion.external <- data.frame(confusion_matrix_results.external$overall)

Accuracy.confusion.external <- AccuracyP.confusion.external[1,]
Sensitivity.confusion.external <- SpecSense.confusion.external[1,]
Specificity.confusion.external <- SpecSense.confusion.external[2,]
Precision.confusion.external <- SpecSense.confusion.external[5,]

```
\newpage
### A confusion matrix is a table of results from cross-validation. Some key metrics provided by a confusion matrix include `Accuracy`, `Precision`, `Sensitivity` and `Specificity`. `Accuracy` is the number of all correct predictions divided by the number of all predictions. `Precision` is the ratio of the number of correctly classified positive predictions divided by the number of all positive predictions. `Sensitivity` (also called `Recall`) is the number of correctly classified positive predictions divided by all positive actuals - this is similiar to precision, except that sensitivity measures against actual values. `Specificity` is the number of correctly classified negative predictions divided by all negative actuals. Simplistically, sensitivity is the true positive rate wherease specificity is the true negative rate. Higher Accuracy, Precision, Sensitivity, and Specificity is desireable.

### Another important component for cross-validation is the `Misclassification Rate`. The Misclassification Rate is a descriptor of how often a model is wrong. This value is equal to the total number of False Positives plus the False Negatives divided by all predictions. A lower misclassification rate is desireable.

```{r Misclassification Calculated from Full Internal Dataset, echo=F, include=F, warning=F}

misclassification.QDA.external <- (confusion_matrix_results.external$table[2,1] + confusion_matrix_results.external$table[1,2]) / sum(confusion_matrix_results.external$table)

misclassification.QDA.internal <- (confusion_matrix_results.internal$table[2,1] + confusion_matrix_results.internal$table[1,2]) / sum(confusion_matrix_results.internal$table)

#misclass <- paste0(round(misclassification.QDA*100, digits=3), "%", sep="")
#misclass <- data.frame(misclass)
#colnames(misclass) <- "Misclassification"
#kable(paste0(round(misclassification.QDA*100, digits=3), "%", sep=""))

#data.frame(misclassification.QDA)

```

### In addition to the misclassification rate, Accuracy, Precision, Sensitivity, Specificity and Misclassification Rate, the Logarithmic Loss function is applied to measure . A lower logarithmic loss value is desireable as logarithmic loss increases as predicted probability diverges from the actual response values and conversely decreases as predicted probability moves converges toward the actual response values.

```{r Log Loss Function, echo=F, include=F, warning=F}

print(logLoss)
logLoss.quadratic <- qda.filtered$results[1,2]
logLoss_StDev <- qda.filtered$results[1,3]

```

### Two final metrics used in this analysis are the `Area Under the Curve (AUC)` and `Receiver Operating Characteristic (ROC) curve`. The ROC bounds an area the area which the AUC describes. As a discrimination threshold changes, the ROC visually represents the correct diagnostic ability of a binary classification model and is a plot of the true positive against the false positive rate at those varied thresholds. As the AUC describes the area under this curve, a higher AUC is more desireable than a lower AUC. As mentioned, these metrics will be analyzed when comparing internal to external cross-validation to ensure consistency as well as between the QDA and Logistic Regression models.

```{r Quadratic Discrim AUC and ROC, echo=F, include=T, warning=F}

dfTrain.numeric$shot_made_flag <- ifelse(dfTrain.numeric$shot_made_flag=="made",1,0)
### Internal Cross-Validation
internal_cv.predicted.qda <- ifelse(internal_cv.predicted.qda=="made",1,0)

AUCpredStep.internal <- prediction(internal_cv.predicted.qda, as.numeric(subDF.Test.numeric$shot_made_flag))
perf_step.internal <- performance(AUCpredStep.internal, measure = "tpr", x.measure = "fpr")
plot(perf_step.internal, main = "ROC Curve - Internal CV")

AUC.internal <- performance(AUCpredStep.internal, measure = "auc")
AUC.internal <- AUC.internal@y.values[[1]]

### External Cross-Validation
external_cv.predicted.qda <- ifelse(external_cv.predicted.qda=="made",1,0)

AUCpredStep.external <- prediction(external_cv.predicted.qda, as.numeric(dfTrain.numeric$shot_made_flag))
perf_step.external <- performance(AUCpredStep.external, measure = "tpr", x.measure = "fpr")
plot(perf_step.external, main = "ROC Curve - External CV")

AUC.external <- performance(AUCpredStep.external, measure = "auc")
AUC.external <- AUC.external@y.values[[1]]

```

# **Quadratic Discriminant Analysis: Internal vs. External Cross-Validation**
### Using the two confusion matrix output tables immediately below, the performance across internal and external cross-validations of the QDA model can be compared. As indicated in those figures, the model performed highly similarly across both cross-validation techniques, indicating the model is consistent and reasonably fit, after controlling for the variables selected for modeling.

```{r Confusion Matrix and Metrics Table, echo=F, include=T, warning=F}

### Internal Cross-Validation Confusion Matrix
confusionFrame.internal <- data.frame(rbind(round(Sensitivity.confusion.internal, digits=5),round(Specificity.confusion.internal, digits=5),round(Precision.confusion.internal, digits=5),round(Accuracy.confusion.internal, digits=5),round(misclassification.QDA.internal, digits=5),round(logLoss.quadratic, digits=5),round(AUC.internal, digits=5)))

rownames(confusionFrame.internal) <- c("Sensitivity","Specificity","Precision","Accuracy","Misclassification Rate","Logarithmic Loss","Area Under the Curve")
colnames(confusionFrame.internal) <- "Internal CV Statistics"

suppressWarnings(kable(confusionFrame.internal,
      format="latex", caption = "Internal Cross-Validation Confusion Matrix", booktabs = T)  %>%
  kable_styling(position = "center"))

### External Cross-Validation Confusion Matrix
confusionFrame.external <- data.frame(rbind(round(Sensitivity.confusion.external, digits=5),round(Specificity.confusion.external, digits=5),round(Precision.confusion.external, digits=5),round(Accuracy.confusion.external, digits=5),round(misclassification.QDA.external, digits=5),round(logLoss.quadratic, digits=5),round(AUC.external, digits=5)))

rownames(confusionFrame.external) <- c("Sensitivity","Specificity","Precision","Accuracy","Misclassification Rate","Logarithmic Loss","Area Under the Curve")
colnames(confusionFrame.external) <- "External CV Statistics"

suppressWarnings(kable(confusionFrame.external,
      format="latex", caption = "External Cross-Validation Confusion Matrix", booktabs = T)  %>%
  kable_styling(position = "center"))

```

```{r Predictions from QDA, echo=F, include=F, warnings=F}

# Apply the developed model to the external data that needs predictions:
pred.qda.filtered <- suppressWarnings(predict(qda.filtered, newdata = df.numeric.preds))

df.preds$shot_made_flag <- pred.qda.filtered

#write.csv(df.preds, "Predicted_Results.csv", row.names = F)

##############################################################################################################
##############################################################################################################
############################################# end of QDA #####################################################
##############################################################################################################
##############################################################################################################

```
\newpage
# Appendix A: Images and Tables
### Correlation Heat Map
```{r Heatmap for Appendix, echo=F, include=T, warning=F}

corrplot::corrplot(cor(df.numeric %>% subset(select=-c(shot_made_flag)))
                   , title = "Correlation among Predictor Variables"
                   , type = "lower"
                   , tl.pos = "ld"
                   , method = "square"
                   , tl.cex = 0.65
                   , tl.col = 'red'
                   , order = "alphabet"
                   , diag = F
                   , mar=c(0,0,5,0)
                   , bg="ivory1"
                   ,tl.srt=.05
)

```

### Correlation Matrix - Top 10 Collinear Terms
```{r Correlation Matrix for Appendix, echo=F, include=T, warning=F}

kable(collinear.correlation_Ten, format="latex", booktabs = T)  %>%
  kable_styling(latex_options="hold_position")

```

### Bartlett Test's Wilks' Lambda Approximation
```{r Bartletts Results for Appendix, echo=F, include=T, warning=F}

kable(bartlettsTest,
      format="latex", caption = "Bartlett Test's Wilks' Lambda Approximation", booktabs = T)  %>%
  kable_styling(position = "center")

```